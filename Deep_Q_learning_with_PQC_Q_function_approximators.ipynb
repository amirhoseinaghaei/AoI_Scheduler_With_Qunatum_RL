{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzTVSOmWSmGTcODdKsJzQH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirhoseinaghaei/AoI_Scheduler_With_Qunatum_RL/blob/main/Deep_Q_learning_with_PQC_Q_function_approximators.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9x7AMJTsTYL",
        "outputId": "556827c1-8857-4f07-eb39-b9149a244591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fusermount: failed to unmount /content/drive: No such file or directory\n",
            "/bin/bash: google-drive-ocamlfuse: command not found\n",
            "Mounted at /content/gdrive/\n",
            "/content/gdrive/MyDrive/Research_Simmulation\n"
          ]
        }
      ],
      "source": [
        "!fusermount -u drive\n",
        "!google-drive-ocamlfuse drive\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive/')\n",
        "%cd gdrive/MyDrive/Research_Simmulation/\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.7.0\n",
        "!pip install tensorflow-quantum==0.7.2\n",
        "!pip install gym==0.18.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9B-DI1DsqUU",
        "outputId": "0c76ffc2-6715-4272-80c9-7a19289b6c07"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.7.0\n",
            "  Downloading tensorflow-2.7.0-cp39-cp39-manylinux2010_x86_64.whl (489.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.7/489.7 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (3.19.6)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (1.4.0)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.1/463.1 KB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (4.5.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (2.11.2)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (0.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (1.6.3)\n",
            "Collecting keras-preprocessing>=1.1.1\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flatbuffers<3.0,>=1.12\n",
            "  Downloading flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n",
            "Collecting keras<2.8,>=2.7.0rc0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (0.40.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (0.2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (15.0.6.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (1.22.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (1.16.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (3.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (0.31.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (2.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0) (1.51.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.2.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.16.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (67.6.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (6.1.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.6->tensorflow==2.7.0) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, flatbuffers, keras-preprocessing, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.11.0\n",
            "    Uninstalling tensorflow-estimator-2.11.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.11.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.11.0\n",
            "    Uninstalling keras-2.11.0:\n",
            "      Successfully uninstalled keras-2.11.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 23.3.3\n",
            "    Uninstalling flatbuffers-23.3.3:\n",
            "      Successfully uninstalled flatbuffers-23.3.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.11.0\n",
            "    Uninstalling tensorflow-2.11.0:\n",
            "      Successfully uninstalled tensorflow-2.11.0\n",
            "Successfully installed flatbuffers-2.0.7 keras-2.7.0 keras-preprocessing-1.1.2 tensorflow-2.7.0 tensorflow-estimator-2.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-quantum==0.7.2\n",
            "  Downloading tensorflow_quantum-0.7.2-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (10.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf==3.17.3\n",
            "  Downloading protobuf-3.17.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sympy==1.8\n",
            "  Downloading sympy-1.8-py3-none-any.whl (6.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-auth==1.18.0\n",
            "  Downloading google_auth-1.18.0-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.9/90.9 KB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-api-core==1.21.0\n",
            "  Downloading google_api_core-1.21.0-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.0/91.0 KB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting googleapis-common-protos==1.52.0\n",
            "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.2/100.2 KB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cirq-google>=0.13.1\n",
            "  Downloading cirq_google-1.1.0-py3-none-any.whl (577 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m577.4/577.4 KB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cirq-core==0.13.1\n",
            "  Downloading cirq_core-0.13.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting networkx~=2.4\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from cirq-core==0.13.1->tensorflow-quantum==0.7.2) (1.4.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from cirq-core==0.13.1->tensorflow-quantum==0.7.2) (1.10.1)\n",
            "Collecting duet~=0.2.0\n",
            "  Downloading duet-0.2.7-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from cirq-core==0.13.1->tensorflow-quantum==0.7.2) (4.65.0)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.9/dist-packages (from cirq-core==0.13.1->tensorflow-quantum==0.7.2) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.16 in /usr/local/lib/python3.9/dist-packages (from cirq-core==0.13.1->tensorflow-quantum==0.7.2) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from cirq-core==0.13.1->tensorflow-quantum==0.7.2) (4.5.0)\n",
            "Requirement already satisfied: sortedcontainers~=2.0 in /usr/local/lib/python3.9/dist-packages (from cirq-core==0.13.1->tensorflow-quantum==0.7.2) (2.4.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.9/dist-packages (from google-api-core==1.21.0->tensorflow-quantum==0.7.2) (2.27.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.9/dist-packages (from google-api-core==1.21.0->tensorflow-quantum==0.7.2) (2022.7.1)\n",
            "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.9/dist-packages (from google-api-core==1.21.0->tensorflow-quantum==0.7.2) (67.6.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from google-api-core==1.21.0->tensorflow-quantum==0.7.2) (1.16.0)\n",
            "Collecting cachetools<5.0,>=2.0.0\n",
            "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth==1.18.0->tensorflow-quantum==0.7.2) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth==1.18.0->tensorflow-quantum==0.7.2) (0.2.8)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy==1.8->tensorflow-quantum==0.7.2) (1.3.0)\n",
            "Requirement already satisfied: proto-plus>=1.20.0 in /usr/local/lib/python3.9/dist-packages (from cirq-google>=0.13.1->tensorflow-quantum==0.7.2) (1.22.2)\n",
            "Collecting cirq-google>=0.13.1\n",
            "  Downloading cirq_google-1.0.0-py3-none-any.whl (576 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m576.5/576.5 KB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading cirq_google-0.15.0-py3-none-any.whl (641 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m641.8/641.8 KB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading cirq_google-0.14.1-py3-none-any.whl (541 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.6/541.6 KB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-api-core[grpc]<2.0.0dev,>=1.14.0\n",
            "  Downloading google_api_core-1.34.0-py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.2/120.2 KB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cirq-google>=0.13.1\n",
            "  Downloading cirq_google-0.14.0-py3-none-any.whl (541 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.6/541.6 KB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading cirq_google-0.13.1-py3-none-any.whl (437 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.1/437.1 KB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-api-core[grpc]<2.0.0dev,>=1.14.0\n",
            "  Downloading google_api_core-1.33.2-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.5/115.5 KB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.33.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.5/115.5 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.33.0-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.2/115.2 KB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.32.0-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.6/93.6 KB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.31.6-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 KB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.31.5-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 KB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.31.4-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 KB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.31.3-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 KB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.31.2-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 KB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.31.1-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.31.0-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.30.0-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.29.0-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 KB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.28.0-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 KB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.27.0-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 KB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.26.3-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 KB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.26.2-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 KB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.26.1-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 KB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.26.0-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 KB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.25.1-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.3/92.3 KB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.25.0-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.3/92.3 KB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.24.1-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 KB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.24.0-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 KB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.23.0-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 KB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.22.4-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.7/91.7 KB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.22.3-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 KB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.22.2-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 KB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.22.1-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 KB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.22.0-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 KB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio<2.0dev,>=1.29.0 in /usr/local/lib/python3.9/dist-packages (from google-api-core==1.21.0->tensorflow-quantum==0.7.2) (1.51.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib~=3.0->cirq-core==0.13.1->tensorflow-quantum==0.7.2) (0.11.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib~=3.0->cirq-core==0.13.1->tensorflow-quantum==0.7.2) (1.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib~=3.0->cirq-core==0.13.1->tensorflow-quantum==0.7.2) (23.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib~=3.0->cirq-core==0.13.1->tensorflow-quantum==0.7.2) (4.39.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib~=3.0->cirq-core==0.13.1->tensorflow-quantum==0.7.2) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib~=3.0->cirq-core==0.13.1->tensorflow-quantum==0.7.2) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib~=3.0->cirq-core==0.13.1->tensorflow-quantum==0.7.2) (1.4.4)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib~=3.0->cirq-core==0.13.1->tensorflow-quantum==0.7.2) (5.12.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib~=3.0->cirq-core==0.13.1->tensorflow-quantum==0.7.2) (8.4.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth==1.18.0->tensorflow-quantum==0.7.2) (0.4.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core==1.21.0->tensorflow-quantum==0.7.2) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core==1.21.0->tensorflow-quantum==0.7.2) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core==1.21.0->tensorflow-quantum==0.7.2) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core==1.21.0->tensorflow-quantum==0.7.2) (3.4)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib~=3.0->cirq-core==0.13.1->tensorflow-quantum==0.7.2) (3.15.0)\n",
            "Installing collected packages: sympy, protobuf, networkx, duet, cachetools, googleapis-common-protos, google-auth, google-api-core, cirq-core, cirq-google, tensorflow-quantum\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.11.1\n",
            "    Uninstalling sympy-1.11.1:\n",
            "      Successfully uninstalled sympy-1.11.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.0\n",
            "    Uninstalling networkx-3.0:\n",
            "      Successfully uninstalled networkx-3.0\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.3.0\n",
            "    Uninstalling cachetools-5.3.0:\n",
            "      Successfully uninstalled cachetools-5.3.0\n",
            "  Attempting uninstall: googleapis-common-protos\n",
            "    Found existing installation: googleapis-common-protos 1.59.0\n",
            "    Uninstalling googleapis-common-protos-1.59.0:\n",
            "      Successfully uninstalled googleapis-common-protos-1.59.0\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.16.2\n",
            "    Uninstalling google-auth-2.16.2:\n",
            "      Successfully uninstalled google-auth-2.16.2\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 2.11.0\n",
            "    Uninstalling google-api-core-2.11.0:\n",
            "      Successfully uninstalled google-api-core-2.11.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-hub 0.13.0 requires protobuf>=3.19.6, but you have protobuf 3.17.3 which is incompatible.\n",
            "pydata-google-auth 1.7.0 requires google-auth<3.0dev,>=1.25.0; python_version >= \"3.6\", but you have google-auth 1.18.0 which is incompatible.\n",
            "proto-plus 1.22.2 requires protobuf<5.0.0dev,>=3.19.0, but you have protobuf 3.17.3 which is incompatible.\n",
            "pandas-gbq 0.17.9 requires google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5, but you have google-api-core 1.21.0 which is incompatible.\n",
            "pandas-gbq 0.17.9 requires google-auth>=1.25.0, but you have google-auth 1.18.0 which is incompatible.\n",
            "google-cloud-translate 3.8.4 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0, but you have google-api-core 1.21.0 which is incompatible.\n",
            "google-cloud-translate 3.8.4 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.17.3 which is incompatible.\n",
            "google-cloud-storage 2.7.0 requires google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5, but you have google-api-core 1.21.0 which is incompatible.\n",
            "google-cloud-storage 2.7.0 requires google-auth<3.0dev,>=1.25.0, but you have google-auth 1.18.0 which is incompatible.\n",
            "google-cloud-language 2.6.1 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0, but you have google-api-core 1.21.0 which is incompatible.\n",
            "google-cloud-language 2.6.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.17.3 which is incompatible.\n",
            "google-cloud-firestore 2.7.3 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 1.21.0 which is incompatible.\n",
            "google-cloud-firestore 2.7.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.17.3 which is incompatible.\n",
            "google-cloud-datastore 2.11.1 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 1.21.0 which is incompatible.\n",
            "google-cloud-datastore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.17.3 which is incompatible.\n",
            "google-cloud-core 2.3.2 requires google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.6, but you have google-api-core 1.21.0 which is incompatible.\n",
            "google-cloud-core 2.3.2 requires google-auth<3.0dev,>=1.25.0, but you have google-auth 1.18.0 which is incompatible.\n",
            "google-cloud-bigquery 3.4.2 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5, but you have google-api-core 1.21.0 which is incompatible.\n",
            "google-cloud-bigquery 3.4.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.17.3 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.19.0 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 1.21.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.17.3 which is incompatible.\n",
            "google-api-python-client 2.70.0 requires google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5, but you have google-api-core 1.21.0 which is incompatible.\n",
            "google-api-python-client 2.70.0 requires google-auth<3.0.0dev,>=1.19.0, but you have google-auth 1.18.0 which is incompatible.\n",
            "firebase-admin 5.3.0 requires google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\", but you have google-api-core 1.21.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cachetools-4.2.4 cirq-core-0.13.1 cirq-google-0.13.1 duet-0.2.7 google-api-core-1.21.0 google-auth-1.18.0 googleapis-common-protos-1.52.0 networkx-2.8.8 protobuf-3.17.3 sympy-1.8 tensorflow-quantum-0.7.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym==0.18.0\n",
            "  Downloading gym-0.18.0.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update package resources to account for version changes.\n",
        "import importlib, pkg_resources\n",
        "importlib.reload(pkg_resources)\n",
        "import tensorflow as tf\n",
        "import tensorflow_quantum as tfq\n",
        "\n",
        "import gym, cirq, sympy\n",
        "import numpy as np\n",
        "from functools import reduce\n",
        "from collections import deque, defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from cirq.contrib.svg import SVGCircuit\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "metadata": {
        "id": "D4W4zrSzsyNG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Rescaling(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Rescaling, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.w = tf.Variable(\n",
        "            initial_value=tf.ones(shape=(1,input_dim)), dtype=\"float32\",\n",
        "            trainable=True, name=\"obs-weights\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.math.multiply((inputs+1)/2, tf.repeat(self.w,repeats=tf.shape(inputs)[0],axis=0))"
      ],
      "metadata": {
        "id": "9uGLwLTCtEjP"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_qubits = 5 # Dimension of the state vectors in CartPole\n",
        "n_layers = 5 # Number of layers in the PQC\n",
        "n_actions = 2 # Number of actions in CartPole\n",
        "\n",
        "qubits = cirq.GridQubit.rect(1, n_qubits)\n",
        "ops = [cirq.Z(q) for q in qubits]\n",
        "observables = [ops[0]*ops[1], ops[2]*ops[3]] # Z_0*Z_1 for action 0 and Z_2*Z_3 for action 1"
      ],
      "metadata": {
        "id": "0NminpPUtpXj"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_qubit_rotation(qubit, symbols):\n",
        "    \"\"\"\n",
        "    Returns Cirq gates that apply a rotation of the bloch sphere about the X,\n",
        "    Y and Z axis, specified by the values in `symbols`.\n",
        "    \"\"\"\n",
        "    return [cirq.rx(symbols[0])(qubit),\n",
        "            cirq.ry(symbols[1])(qubit),\n",
        "            cirq.rz(symbols[2])(qubit)]\n",
        "\n",
        "def entangling_layer(qubits):\n",
        "    \"\"\"\n",
        "    Returns a layer of CZ entangling gates on `qubits` (arranged in a circular topology).\n",
        "    \"\"\"\n",
        "    cz_ops = [cirq.CZ(q0, q1) for q0, q1 in zip(qubits, qubits[1:])]\n",
        "    cz_ops += ([cirq.CZ(qubits[0], qubits[-1])] if len(qubits) != 2 else [])\n",
        "    return cz_ops\n",
        "def generate_circuit(qubits, n_layers):\n",
        "    \"\"\"Prepares a data re-uploading circuit on `qubits` with `n_layers` layers.\"\"\"\n",
        "    # Number of qubits\n",
        "    n_qubits = len(qubits)\n",
        "    \n",
        "    # Sympy symbols for variational angles\n",
        "    params = sympy.symbols(f'theta(0:{3*(n_layers+1)*n_qubits})')\n",
        "    params = np.asarray(params).reshape((n_layers + 1, n_qubits, 3))\n",
        "    \n",
        "    # Sympy symbols for encoding angles\n",
        "    inputs = sympy.symbols(f'x(0:{n_layers})'+f'_(0:{n_qubits})')\n",
        "    inputs = np.asarray(inputs).reshape((n_layers, n_qubits))\n",
        "    \n",
        "    # Define circuit\n",
        "    circuit = cirq.Circuit()\n",
        "    for l in range(n_layers):\n",
        "        # Variational layer\n",
        "        circuit += cirq.Circuit(one_qubit_rotation(q, params[l, i]) for i, q in enumerate(qubits))\n",
        "        circuit += entangling_layer(qubits)\n",
        "        # Encoding layer\n",
        "        circuit += cirq.Circuit(cirq.rx(inputs[l, i])(q) for i, q in enumerate(qubits))\n",
        "\n",
        "    # Last varitional layer\n",
        "    circuit += cirq.Circuit(one_qubit_rotation(q, params[n_layers, i]) for i,q in enumerate(qubits))\n",
        "    \n",
        "    return circuit, list(params.flat), list(inputs.flat)"
      ],
      "metadata": {
        "id": "v8KRsmhGuM8a"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReUploadingPQC(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Performs the transformation (s_1, ..., s_d) -> (theta_1, ..., theta_N, lmbd[1][1]s_1, ..., lmbd[1][M]s_1,\n",
        "        ......., lmbd[d][1]s_d, ..., lmbd[d][M]s_d) for d=input_dim, N=theta_dim and M=n_layers.\n",
        "    An activation function from tf.keras.activations, specified by `activation` ('linear' by default) is\n",
        "        then applied to all lmbd[i][j]s_i.\n",
        "    All angles are finally permuted to follow the alphabetical order of their symbol names, as processed\n",
        "        by the ControlledPQC.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, qubits, n_layers, observables, activation=\"linear\", name=\"re-uploading_PQC\"):\n",
        "        super(ReUploadingPQC, self).__init__(name=name)\n",
        "        self.n_layers = n_layers\n",
        "        self.n_qubits = len(qubits)\n",
        "\n",
        "        circuit, theta_symbols, input_symbols = generate_circuit(qubits, n_layers)\n",
        "\n",
        "        theta_init = tf.random_uniform_initializer(minval=0.0, maxval=np.pi)\n",
        "        self.theta = tf.Variable(\n",
        "            initial_value=theta_init(shape=(1, len(theta_symbols)), dtype=\"float32\"),\n",
        "            trainable=True, name=\"thetas\"\n",
        "        )\n",
        "        \n",
        "        lmbd_init = tf.ones(shape=(self.n_qubits * self.n_layers,))\n",
        "        self.lmbd = tf.Variable(\n",
        "            initial_value=lmbd_init, dtype=\"float32\", trainable=True, name=\"lambdas\"\n",
        "        )\n",
        "        \n",
        "        # Define explicit symbol order.\n",
        "        symbols = [str(symb) for symb in theta_symbols + input_symbols]\n",
        "        self.indices = tf.constant([symbols.index(a) for a in sorted(symbols)])\n",
        "        \n",
        "        self.activation = activation\n",
        "        self.empty_circuit = tfq.convert_to_tensor([cirq.Circuit()])\n",
        "        self.computation_layer = tfq.layers.ControlledPQC(circuit, observables)        \n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs[0] = encoding data for the state.\n",
        "        batch_dim = tf.gather(tf.shape(inputs[0]), 0)\n",
        "        tiled_up_circuits = tf.repeat(self.empty_circuit, repeats=batch_dim)\n",
        "        tiled_up_thetas = tf.tile(self.theta, multiples=[batch_dim, 1])\n",
        "        tiled_up_inputs = tf.tile(inputs[0], multiples=[1, self.n_layers])\n",
        "        scaled_inputs = tf.einsum(\"i,ji->ji\", self.lmbd, tiled_up_inputs)\n",
        "        squashed_inputs = tf.keras.layers.Activation(self.activation)(scaled_inputs)\n",
        "\n",
        "        joined_vars = tf.concat([tiled_up_thetas, squashed_inputs], axis=1)\n",
        "        joined_vars = tf.gather(joined_vars, self.indices, axis=1)\n",
        "        \n",
        "        return self.computation_layer([tiled_up_circuits, joined_vars])"
      ],
      "metadata": {
        "id": "n8sLi72juC_M"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_model_Qlearning(qubits, n_layers, n_actions, observables, target):\n",
        "    \"\"\"Generates a Keras model for a data re-uploading PQC Q-function approximator.\"\"\"\n",
        "\n",
        "    input_tensor = tf.keras.Input(shape=(len(qubits), ), dtype=tf.dtypes.float32, name='input')\n",
        "    re_uploading_pqc = ReUploadingPQC(qubits, n_layers, observables, activation='tanh')([input_tensor])\n",
        "    process = tf.keras.Sequential([Rescaling(len(observables))], name=target*\"Target\"+\"Q-values\")\n",
        "    Q_values = process(re_uploading_pqc)\n",
        "    model = tf.keras.Model(inputs=[input_tensor], outputs=Q_values)\n",
        "\n",
        "    return model\n",
        "\n",
        "model = generate_model_Qlearning(qubits, n_layers, n_actions, observables, False)\n",
        "model_target = generate_model_Qlearning(qubits, n_layers, n_actions, observables, True)\n",
        "\n",
        "model_target.set_weights(model.get_weights())"
      ],
      "metadata": {
        "id": "r1uFD2y4t35N"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def Q_learning_update(states, actions, rewards, next_states, done, model, gamma, n_actions):\n",
        "    states = tf.convert_to_tensor(states)\n",
        "    actions = tf.convert_to_tensor(actions)\n",
        "    rewards = tf.convert_to_tensor(rewards)\n",
        "    next_states = tf.convert_to_tensor(next_states)\n",
        "    done = tf.convert_to_tensor(done)\n",
        "\n",
        "    # Compute their target q_values and the masks on sampled actions\n",
        "    future_rewards = model_target([next_states])\n",
        "    target_q_values = rewards + (gamma * tf.reduce_max(future_rewards, axis=1)\n",
        "                                                   * (1.0 - done))\n",
        "    masks = tf.one_hot(actions, n_actions)\n",
        "\n",
        "    # Train the model on the states and target Q-values\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(model.trainable_variables)\n",
        "        q_values = model([states])\n",
        "        q_values_masked = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
        "        loss = tf.keras.losses.Huber()(target_q_values, q_values_masked)\n",
        "\n",
        "    # Backpropagation\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    for optimizer, w in zip([optimizer_in, optimizer_var, optimizer_out], [w_in, w_var, w_out]):\n",
        "        optimizer.apply_gradients([(grads[w], model.trainable_variables[w])])"
      ],
      "metadata": {
        "id": "eRJsyq9_vGaM"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "gamma = 1\n",
        "n_episodes = 5000\n",
        "\n",
        "# Define replay memory\n",
        "max_memory_length = 10000 # Maximum replay length\n",
        "replay_memory = deque(maxlen=max_memory_length)\n",
        "\n",
        "epsilon = 0.7  # Epsilon greedy parameter\n",
        "epsilon_min = 0.3 # Minimum epsilon greedy parameter\n",
        "decay_epsilon = 0.99 # Decay rate of epsilon greedy parameter\n",
        "batch_size = 32\n",
        "steps_per_update = 10 # Train the model every x steps\n",
        "steps_per_target_update = 30 # Update the target model every x steps"
      ],
      "metadata": {
        "id": "lqTqpdDivKv0"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_in = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
        "optimizer_var = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
        "optimizer_out = tf.keras.optimizers.Adam(learning_rate=0.1, amsgrad=True)\n",
        "\n",
        "# Assign the model parameters to each optimizer\n",
        "w_in, w_var, w_out = 1, 0, 2"
      ],
      "metadata": {
        "id": "KxsovBtuvPel"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time \n",
        "import numpy as np\n",
        "from Environment import Environment \n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from collections import deque, defaultdict\n",
        "\n",
        "env = Environment(1000,100)\n",
        "env.CreateStates()\n",
        "episode_reward_history = []\n",
        "step_count = 0\n",
        "for episode in range(n_episodes):\n",
        "    episode_reward = 0\n",
        "    env.reset_paramter()\n",
        "    state, _ = env.reset_state()\n",
        "    env.generate_channel_state_list_for_whole_sequence(state[1])\n",
        "    if state[1] == \"Ch1\":\n",
        "        state[1] = 1\n",
        "    else:\n",
        "        state[1] = 0\n",
        "    state = state.astype(np.float32)\n",
        "\n",
        "    while True:\n",
        "        # Interact with env\n",
        "        interaction = interact_env(state, model, epsilon, n_actions, env)\n",
        "\n",
        "        # Store interaction in the replay memory\n",
        "        replay_memory.append(interaction)\n",
        "\n",
        "        state = interaction['next_state']\n",
        "        episode_reward += interaction['reward']\n",
        "        step_count += 1\n",
        "\n",
        "        # Update model\n",
        "        if step_count % steps_per_update == 0:\n",
        "            # Sample a batch of interactions and update Q_function\n",
        "            training_batch = np.random.choice(replay_memory, size=batch_size)\n",
        "            Q_learning_update(np.asarray([x['state'] for x in training_batch]),\n",
        "                              np.asarray([x['action'] for x in training_batch]),\n",
        "                              np.asarray([x['reward'] for x in training_batch], dtype=np.float32),\n",
        "                              np.asarray([x['next_state'] for x in training_batch]),\n",
        "                              np.asarray([x['done'] for x in training_batch], dtype=np.float32),\n",
        "                              model, gamma, n_actions)\n",
        "\n",
        "        # Update target model\n",
        "        if step_count % steps_per_target_update == 0:\n",
        "            model_target.set_weights(model.get_weights())\n",
        "\n",
        "        # Check if the episode is finished\n",
        "        if interaction['done']:\n",
        "            break\n",
        "\n",
        "    # Decay epsilon\n",
        "    if episode % 10 == 0:\n",
        "      epsilon = max(epsilon * decay_epsilon, epsilon_min)\n",
        "      # print(epsilon)\n",
        "    episode_reward_history.append(episode_reward)\n",
        "    if (episode+1)%10 == 0:\n",
        "        avg_rewards = np.mean(episode_reward_history[-10:])\n",
        "        print(\"Episode {}/{}, average last 10 rewards {}\".format(\n",
        "            episode+1, n_episodes, avg_rewards))\n",
        "        # if avg_rewards >= 500.0:\n",
        "        #     break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfwZRw4UvlWN",
        "outputId": "7e4a1600-625a-45a7-9fb1-649603b62f63"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 10/5000, average last 10 rewards -470.2\n",
            "Episode 20/5000, average last 10 rewards 60.0\n",
            "Episode 30/5000, average last 10 rewards 370.4\n",
            "Episode 40/5000, average last 10 rewards 17.8\n",
            "Episode 50/5000, average last 10 rewards 540.0\n",
            "Episode 60/5000, average last 10 rewards -25.0\n",
            "Episode 70/5000, average last 10 rewards 108.6\n",
            "Episode 80/5000, average last 10 rewards -661.0\n",
            "Episode 90/5000, average last 10 rewards -312.0\n",
            "Episode 100/5000, average last 10 rewards -144.0\n",
            "Episode 110/5000, average last 10 rewards -294.0\n",
            "Episode 120/5000, average last 10 rewards 29.0\n",
            "Episode 130/5000, average last 10 rewards 113.6\n",
            "Episode 140/5000, average last 10 rewards 876.8\n",
            "Episode 150/5000, average last 10 rewards 23.0\n",
            "Episode 160/5000, average last 10 rewards -96.0\n",
            "Episode 170/5000, average last 10 rewards 168.8\n",
            "Episode 180/5000, average last 10 rewards 215.6\n",
            "Episode 190/5000, average last 10 rewards -523.0\n",
            "Episode 200/5000, average last 10 rewards 35.0\n",
            "Episode 210/5000, average last 10 rewards 72.0\n",
            "Episode 220/5000, average last 10 rewards -762.0\n",
            "Episode 230/5000, average last 10 rewards -60.0\n",
            "Episode 240/5000, average last 10 rewards 102.0\n",
            "Episode 250/5000, average last 10 rewards -486.0\n",
            "Episode 260/5000, average last 10 rewards 120.2\n",
            "Episode 270/5000, average last 10 rewards -564.8\n",
            "Episode 280/5000, average last 10 rewards -229.2\n",
            "Episode 290/5000, average last 10 rewards -798.0\n",
            "Episode 300/5000, average last 10 rewards 1249.0\n",
            "Episode 310/5000, average last 10 rewards -162.0\n",
            "Episode 320/5000, average last 10 rewards -72.8\n",
            "Episode 330/5000, average last 10 rewards -24.0\n",
            "Episode 340/5000, average last 10 rewards -589.0\n",
            "Episode 350/5000, average last 10 rewards -720.0\n",
            "Episode 360/5000, average last 10 rewards -750.6\n",
            "Episode 370/5000, average last 10 rewards 474.0\n",
            "Episode 380/5000, average last 10 rewards 414.8\n",
            "Episode 390/5000, average last 10 rewards -241.8\n",
            "Episode 400/5000, average last 10 rewards 253.4\n",
            "Episode 410/5000, average last 10 rewards 514.2\n",
            "Episode 420/5000, average last 10 rewards 11.0\n",
            "Episode 430/5000, average last 10 rewards -332.0\n",
            "Episode 440/5000, average last 10 rewards 1319.2\n",
            "Episode 450/5000, average last 10 rewards -438.0\n",
            "Episode 460/5000, average last 10 rewards 78.0\n",
            "Episode 470/5000, average last 10 rewards 210.0\n",
            "Episode 480/5000, average last 10 rewards -786.8\n",
            "Episode 490/5000, average last 10 rewards -626.4\n",
            "Episode 500/5000, average last 10 rewards -234.0\n",
            "Episode 510/5000, average last 10 rewards 101.0\n",
            "Episode 520/5000, average last 10 rewards -379.0\n",
            "Episode 530/5000, average last 10 rewards 432.0\n",
            "Episode 540/5000, average last 10 rewards -210.8\n",
            "Episode 550/5000, average last 10 rewards 390.0\n",
            "Episode 560/5000, average last 10 rewards 568.8\n",
            "Episode 570/5000, average last 10 rewards -750.0\n",
            "Episode 580/5000, average last 10 rewards 492.0\n",
            "Episode 590/5000, average last 10 rewards -492.0\n",
            "Episode 600/5000, average last 10 rewards -102.0\n",
            "Episode 610/5000, average last 10 rewards -227.2\n",
            "Episode 620/5000, average last 10 rewards -684.0\n",
            "Episode 630/5000, average last 10 rewards -912.0\n",
            "Episode 640/5000, average last 10 rewards -162.0\n",
            "Episode 650/5000, average last 10 rewards -1014.0\n",
            "Episode 660/5000, average last 10 rewards -408.0\n",
            "Episode 670/5000, average last 10 rewards -678.0\n",
            "Episode 680/5000, average last 10 rewards -330.0\n",
            "Episode 690/5000, average last 10 rewards 113.0\n",
            "Episode 700/5000, average last 10 rewards 222.0\n",
            "Episode 710/5000, average last 10 rewards 61.2\n",
            "Episode 720/5000, average last 10 rewards -198.0\n",
            "Episode 730/5000, average last 10 rewards -816.0\n",
            "Episode 740/5000, average last 10 rewards -181.0\n",
            "Episode 750/5000, average last 10 rewards -186.8\n",
            "Episode 760/5000, average last 10 rewards -138.0\n",
            "Episode 770/5000, average last 10 rewards 401.6\n",
            "Episode 780/5000, average last 10 rewards 215.2\n",
            "Episode 790/5000, average last 10 rewards -389.2\n",
            "Episode 800/5000, average last 10 rewards -72.0\n",
            "Episode 810/5000, average last 10 rewards -1093.0\n",
            "Episode 820/5000, average last 10 rewards 329.2\n",
            "Episode 830/5000, average last 10 rewards -223.0\n",
            "Episode 840/5000, average last 10 rewards -280.8\n",
            "Episode 850/5000, average last 10 rewards 372.0\n",
            "Episode 860/5000, average last 10 rewards -702.0\n",
            "Episode 870/5000, average last 10 rewards -656.0\n",
            "Episode 880/5000, average last 10 rewards 336.0\n",
            "Episode 890/5000, average last 10 rewards -455.2\n",
            "Episode 900/5000, average last 10 rewards 587.8\n",
            "Episode 910/5000, average last 10 rewards -182.2\n",
            "Episode 920/5000, average last 10 rewards -336.8\n",
            "Episode 930/5000, average last 10 rewards 1746.0\n",
            "Episode 940/5000, average last 10 rewards -42.0\n",
            "Episode 950/5000, average last 10 rewards -61.6\n",
            "Episode 960/5000, average last 10 rewards -36.6\n",
            "Episode 970/5000, average last 10 rewards -1099.0\n",
            "Episode 980/5000, average last 10 rewards -209.0\n",
            "Episode 990/5000, average last 10 rewards -656.6\n",
            "Episode 1000/5000, average last 10 rewards 120.4\n",
            "Episode 1010/5000, average last 10 rewards 401.6\n",
            "Episode 1020/5000, average last 10 rewards 190.8\n",
            "Episode 1030/5000, average last 10 rewards 53.2\n",
            "Episode 1040/5000, average last 10 rewards -287.4\n",
            "Episode 1050/5000, average last 10 rewards -204.0\n",
            "Episode 1060/5000, average last 10 rewards -636.0\n",
            "Episode 1070/5000, average last 10 rewards 401.0\n",
            "Episode 1080/5000, average last 10 rewards 474.0\n",
            "Episode 1090/5000, average last 10 rewards 132.0\n",
            "Episode 1100/5000, average last 10 rewards 641.0\n",
            "Episode 1110/5000, average last 10 rewards 720.0\n",
            "Episode 1120/5000, average last 10 rewards 30.0\n",
            "Episode 1130/5000, average last 10 rewards 462.8\n",
            "Episode 1140/5000, average last 10 rewards 570.0\n",
            "Episode 1150/5000, average last 10 rewards -666.0\n",
            "Episode 1160/5000, average last 10 rewards 78.0\n",
            "Episode 1170/5000, average last 10 rewards -402.0\n",
            "Episode 1180/5000, average last 10 rewards -12.0\n",
            "Episode 1190/5000, average last 10 rewards -252.0\n",
            "Episode 1200/5000, average last 10 rewards 724.2\n",
            "Episode 1210/5000, average last 10 rewards -95.0\n",
            "Episode 1220/5000, average last 10 rewards -156.2\n",
            "Episode 1230/5000, average last 10 rewards -870.0\n",
            "Episode 1240/5000, average last 10 rewards 563.8\n",
            "Episode 1250/5000, average last 10 rewards -342.0\n",
            "Episode 1260/5000, average last 10 rewards 244.6\n",
            "Episode 1270/5000, average last 10 rewards 763.2\n",
            "Episode 1280/5000, average last 10 rewards -91.0\n",
            "Episode 1290/5000, average last 10 rewards 173.2\n",
            "Episode 1300/5000, average last 10 rewards 740.0\n",
            "Episode 1310/5000, average last 10 rewards -252.0\n",
            "Episode 1320/5000, average last 10 rewards 240.0\n",
            "Episode 1330/5000, average last 10 rewards -557.0\n",
            "Episode 1340/5000, average last 10 rewards 486.0\n",
            "Episode 1350/5000, average last 10 rewards -858.0\n",
            "Episode 1360/5000, average last 10 rewards -672.0\n",
            "Episode 1370/5000, average last 10 rewards -353.0\n",
            "Episode 1380/5000, average last 10 rewards -102.0\n",
            "Episode 1390/5000, average last 10 rewards -192.0\n",
            "Episode 1400/5000, average last 10 rewards -40.8\n",
            "Episode 1410/5000, average last 10 rewards 576.0\n",
            "Episode 1420/5000, average last 10 rewards -36.0\n",
            "Episode 1430/5000, average last 10 rewards 1140.0\n",
            "Episode 1440/5000, average last 10 rewards 96.0\n",
            "Episode 1450/5000, average last 10 rewards 408.8\n",
            "Episode 1460/5000, average last 10 rewards 307.0\n",
            "Episode 1470/5000, average last 10 rewards 246.0\n",
            "Episode 1480/5000, average last 10 rewards -66.0\n",
            "Episode 1490/5000, average last 10 rewards 828.0\n",
            "Episode 1500/5000, average last 10 rewards -1182.0\n",
            "Episode 1510/5000, average last 10 rewards -101.6\n",
            "Episode 1520/5000, average last 10 rewards 270.0\n",
            "Episode 1530/5000, average last 10 rewards 468.8\n",
            "Episode 1540/5000, average last 10 rewards -330.0\n",
            "Episode 1550/5000, average last 10 rewards 156.6\n",
            "Episode 1560/5000, average last 10 rewards -378.0\n",
            "Episode 1570/5000, average last 10 rewards -515.2\n",
            "Episode 1580/5000, average last 10 rewards -120.0\n",
            "Episode 1590/5000, average last 10 rewards 396.2\n",
            "Episode 1600/5000, average last 10 rewards 281.2\n",
            "Episode 1610/5000, average last 10 rewards -100.4\n",
            "Episode 1620/5000, average last 10 rewards -1194.0\n",
            "Episode 1630/5000, average last 10 rewards 433.2\n",
            "Episode 1640/5000, average last 10 rewards 211.2\n",
            "Episode 1650/5000, average last 10 rewards 144.0\n",
            "Episode 1660/5000, average last 10 rewards 630.0\n",
            "Episode 1670/5000, average last 10 rewards 571.8\n",
            "Episode 1680/5000, average last 10 rewards -1006.8\n",
            "Episode 1690/5000, average last 10 rewards -153.8\n",
            "Episode 1700/5000, average last 10 rewards 222.0\n",
            "Episode 1710/5000, average last 10 rewards 199.6\n",
            "Episode 1720/5000, average last 10 rewards 35.0\n",
            "Episode 1730/5000, average last 10 rewards -288.0\n",
            "Episode 1740/5000, average last 10 rewards 6.0\n",
            "Episode 1750/5000, average last 10 rewards 535.2\n",
            "Episode 1760/5000, average last 10 rewards 0.0\n",
            "Episode 1770/5000, average last 10 rewards -186.0\n",
            "Episode 1780/5000, average last 10 rewards 186.0\n",
            "Episode 1790/5000, average last 10 rewards 253.2\n",
            "Episode 1800/5000, average last 10 rewards 456.0\n",
            "Episode 1810/5000, average last 10 rewards 79.8\n",
            "Episode 1820/5000, average last 10 rewards 194.6\n",
            "Episode 1830/5000, average last 10 rewards 343.2\n",
            "Episode 1840/5000, average last 10 rewards -24.0\n",
            "Episode 1850/5000, average last 10 rewards -113.0\n",
            "Episode 1860/5000, average last 10 rewards -309.0\n",
            "Episode 1870/5000, average last 10 rewards 72.8\n",
            "Episode 1880/5000, average last 10 rewards 228.4\n",
            "Episode 1890/5000, average last 10 rewards 42.0\n",
            "Episode 1900/5000, average last 10 rewards 642.0\n",
            "Episode 1910/5000, average last 10 rewards 943.0\n",
            "Episode 1920/5000, average last 10 rewards 697.0\n",
            "Episode 1930/5000, average last 10 rewards 510.0\n",
            "Episode 1940/5000, average last 10 rewards -534.0\n",
            "Episode 1950/5000, average last 10 rewards 150.0\n",
            "Episode 1960/5000, average last 10 rewards 1.0\n",
            "Episode 1970/5000, average last 10 rewards -222.0\n",
            "Episode 1980/5000, average last 10 rewards -367.2\n",
            "Episode 1990/5000, average last 10 rewards 197.4\n",
            "Episode 2000/5000, average last 10 rewards 480.0\n",
            "Episode 2010/5000, average last 10 rewards 348.6\n",
            "Episode 2020/5000, average last 10 rewards 288.8\n",
            "Episode 2030/5000, average last 10 rewards 30.0\n",
            "Episode 2040/5000, average last 10 rewards 181.4\n",
            "Episode 2050/5000, average last 10 rewards 612.0\n",
            "Episode 2060/5000, average last 10 rewards -157.2\n",
            "Episode 2070/5000, average last 10 rewards 246.0\n",
            "Episode 2080/5000, average last 10 rewards 528.6\n",
            "Episode 2090/5000, average last 10 rewards 228.0\n",
            "Episode 2100/5000, average last 10 rewards -965.0\n",
            "Episode 2110/5000, average last 10 rewards -851.0\n",
            "Episode 2120/5000, average last 10 rewards 258.0\n",
            "Episode 2130/5000, average last 10 rewards -422.0\n",
            "Episode 2140/5000, average last 10 rewards 450.0\n",
            "Episode 2150/5000, average last 10 rewards 618.0\n",
            "Episode 2160/5000, average last 10 rewards 168.0\n",
            "Episode 2170/5000, average last 10 rewards -89.2\n",
            "Episode 2180/5000, average last 10 rewards 90.4\n",
            "Episode 2190/5000, average last 10 rewards 175.2\n",
            "Episode 2200/5000, average last 10 rewards -102.0\n",
            "Episode 2210/5000, average last 10 rewards -120.0\n",
            "Episode 2220/5000, average last 10 rewards 119.2\n",
            "Episode 2230/5000, average last 10 rewards 96.0\n",
            "Episode 2240/5000, average last 10 rewards 450.0\n",
            "Episode 2250/5000, average last 10 rewards 126.0\n",
            "Episode 2260/5000, average last 10 rewards -209.2\n",
            "Episode 2270/5000, average last 10 rewards 618.0\n",
            "Episode 2280/5000, average last 10 rewards -1.2\n",
            "Episode 2290/5000, average last 10 rewards 690.0\n",
            "Episode 2300/5000, average last 10 rewards 474.8\n",
            "Episode 2310/5000, average last 10 rewards -306.0\n",
            "Episode 2320/5000, average last 10 rewards 583.0\n",
            "Episode 2330/5000, average last 10 rewards 36.0\n",
            "Episode 2340/5000, average last 10 rewards -564.0\n",
            "Episode 2350/5000, average last 10 rewards 522.0\n",
            "Episode 2360/5000, average last 10 rewards 510.0\n",
            "Episode 2370/5000, average last 10 rewards 402.0\n",
            "Episode 2380/5000, average last 10 rewards -125.4\n",
            "Episode 2390/5000, average last 10 rewards -630.0\n",
            "Episode 2400/5000, average last 10 rewards -737.4\n",
            "Episode 2410/5000, average last 10 rewards 252.6\n",
            "Episode 2420/5000, average last 10 rewards 684.8\n",
            "Episode 2430/5000, average last 10 rewards 90.8\n",
            "Episode 2440/5000, average last 10 rewards 391.0\n",
            "Episode 2450/5000, average last 10 rewards -84.0\n",
            "Episode 2460/5000, average last 10 rewards -149.2\n",
            "Episode 2470/5000, average last 10 rewards 264.0\n",
            "Episode 2480/5000, average last 10 rewards 566.4\n",
            "Episode 2490/5000, average last 10 rewards 54.0\n",
            "Episode 2500/5000, average last 10 rewards -64.8\n",
            "Episode 2510/5000, average last 10 rewards 24.4\n",
            "Episode 2520/5000, average last 10 rewards 420.4\n",
            "Episode 2530/5000, average last 10 rewards 529.0\n",
            "Episode 2540/5000, average last 10 rewards 703.0\n",
            "Episode 2550/5000, average last 10 rewards -90.0\n",
            "Episode 2560/5000, average last 10 rewards 529.2\n",
            "Episode 2570/5000, average last 10 rewards 859.0\n",
            "Episode 2580/5000, average last 10 rewards 511.0\n",
            "Episode 2590/5000, average last 10 rewards 396.0\n",
            "Episode 2600/5000, average last 10 rewards -279.8\n",
            "Episode 2610/5000, average last 10 rewards -18.0\n",
            "Episode 2620/5000, average last 10 rewards -156.0\n",
            "Episode 2630/5000, average last 10 rewards -436.8\n",
            "Episode 2640/5000, average last 10 rewards 762.6\n",
            "Episode 2650/5000, average last 10 rewards 7.6\n",
            "Episode 2660/5000, average last 10 rewards -161.0\n",
            "Episode 2670/5000, average last 10 rewards 930.0\n",
            "Episode 2680/5000, average last 10 rewards -162.0\n",
            "Episode 2690/5000, average last 10 rewards -99.8\n",
            "Episode 2700/5000, average last 10 rewards 307.0\n",
            "Episode 2710/5000, average last 10 rewards -282.0\n",
            "Episode 2720/5000, average last 10 rewards 528.0\n",
            "Episode 2730/5000, average last 10 rewards 6.0\n",
            "Episode 2740/5000, average last 10 rewards 762.0\n",
            "Episode 2750/5000, average last 10 rewards 229.2\n",
            "Episode 2760/5000, average last 10 rewards -65.0\n",
            "Episode 2770/5000, average last 10 rewards 685.4\n",
            "Episode 2780/5000, average last 10 rewards -755.4\n",
            "Episode 2790/5000, average last 10 rewards 528.0\n",
            "Episode 2800/5000, average last 10 rewards -46.4\n",
            "Episode 2810/5000, average last 10 rewards -186.0\n",
            "Episode 2820/5000, average last 10 rewards -245.0\n",
            "Episode 2830/5000, average last 10 rewards -78.0\n",
            "Episode 2840/5000, average last 10 rewards -48.0\n",
            "Episode 2850/5000, average last 10 rewards 379.8\n",
            "Episode 2860/5000, average last 10 rewards 235.8\n",
            "Episode 2870/5000, average last 10 rewards 84.0\n",
            "Episode 2880/5000, average last 10 rewards 2141.6\n",
            "Episode 2890/5000, average last 10 rewards -456.4\n",
            "Episode 2900/5000, average last 10 rewards 799.0\n",
            "Episode 2910/5000, average last 10 rewards 337.6\n",
            "Episode 2920/5000, average last 10 rewards 545.0\n",
            "Episode 2930/5000, average last 10 rewards 612.0\n",
            "Episode 2940/5000, average last 10 rewards 168.4\n",
            "Episode 2950/5000, average last 10 rewards 120.0\n",
            "Episode 2960/5000, average last 10 rewards -167.2\n",
            "Episode 2970/5000, average last 10 rewards -89.2\n",
            "Episode 2980/5000, average last 10 rewards 156.0\n",
            "Episode 2990/5000, average last 10 rewards -95.4\n",
            "Episode 3000/5000, average last 10 rewards -651.6\n",
            "Episode 3010/5000, average last 10 rewards -569.0\n",
            "Episode 3020/5000, average last 10 rewards 194.2\n",
            "Episode 3030/5000, average last 10 rewards 612.0\n",
            "Episode 3040/5000, average last 10 rewards 396.0\n",
            "Episode 3050/5000, average last 10 rewards -312.0\n",
            "Episode 3060/5000, average last 10 rewards 396.0\n",
            "Episode 3070/5000, average last 10 rewards 66.0\n",
            "Episode 3080/5000, average last 10 rewards 169.0\n",
            "Episode 3090/5000, average last 10 rewards -102.0\n",
            "Episode 3100/5000, average last 10 rewards 126.0\n",
            "Episode 3110/5000, average last 10 rewards 240.0\n",
            "Episode 3120/5000, average last 10 rewards 372.0\n",
            "Episode 3130/5000, average last 10 rewards 215.2\n",
            "Episode 3140/5000, average last 10 rewards 235.0\n",
            "Episode 3150/5000, average last 10 rewards 403.4\n",
            "Episode 3160/5000, average last 10 rewards 235.2\n",
            "Episode 3170/5000, average last 10 rewards -318.0\n",
            "Episode 3180/5000, average last 10 rewards -1212.0\n",
            "Episode 3190/5000, average last 10 rewards 119.2\n",
            "Episode 3200/5000, average last 10 rewards -261.8\n",
            "Episode 3210/5000, average last 10 rewards 216.6\n",
            "Episode 3220/5000, average last 10 rewards -324.0\n",
            "Episode 3230/5000, average last 10 rewards 1170.2\n",
            "Episode 3240/5000, average last 10 rewards 306.0\n",
            "Episode 3250/5000, average last 10 rewards 43.6\n",
            "Episode 3260/5000, average last 10 rewards 720.0\n",
            "Episode 3270/5000, average last 10 rewards 486.0\n",
            "Episode 3280/5000, average last 10 rewards -396.0\n",
            "Episode 3290/5000, average last 10 rewards 696.0\n",
            "Episode 3300/5000, average last 10 rewards 264.0\n",
            "Episode 3310/5000, average last 10 rewards 114.0\n",
            "Episode 3320/5000, average last 10 rewards 480.8\n",
            "Episode 3330/5000, average last 10 rewards 1002.0\n",
            "Episode 3340/5000, average last 10 rewards -252.4\n",
            "Episode 3350/5000, average last 10 rewards -173.0\n",
            "Episode 3360/5000, average last 10 rewards 588.0\n",
            "Episode 3370/5000, average last 10 rewards 120.0\n",
            "Episode 3380/5000, average last 10 rewards 661.0\n",
            "Episode 3390/5000, average last 10 rewards 708.0\n",
            "Episode 3400/5000, average last 10 rewards 54.0\n",
            "Episode 3410/5000, average last 10 rewards 133.8\n",
            "Episode 3420/5000, average last 10 rewards 97.0\n",
            "Episode 3430/5000, average last 10 rewards -210.0\n",
            "Episode 3440/5000, average last 10 rewards -540.0\n",
            "Episode 3450/5000, average last 10 rewards 708.0\n",
            "Episode 3460/5000, average last 10 rewards 785.2\n",
            "Episode 3470/5000, average last 10 rewards -258.0\n",
            "Episode 3480/5000, average last 10 rewards 852.0\n",
            "Episode 3490/5000, average last 10 rewards 499.2\n",
            "Episode 3500/5000, average last 10 rewards 320.0\n",
            "Episode 3510/5000, average last 10 rewards -184.4\n",
            "Episode 3520/5000, average last 10 rewards 546.6\n",
            "Episode 3530/5000, average last 10 rewards 258.0\n",
            "Episode 3540/5000, average last 10 rewards 378.8\n",
            "Episode 3550/5000, average last 10 rewards 1111.6\n",
            "Episode 3560/5000, average last 10 rewards 583.0\n",
            "Episode 3570/5000, average last 10 rewards -630.0\n",
            "Episode 3580/5000, average last 10 rewards -204.0\n",
            "Episode 3590/5000, average last 10 rewards 684.0\n",
            "Episode 3600/5000, average last 10 rewards 984.0\n",
            "Episode 3610/5000, average last 10 rewards -94.0\n",
            "Episode 3620/5000, average last 10 rewards -388.0\n",
            "Episode 3630/5000, average last 10 rewards 138.0\n",
            "Episode 3640/5000, average last 10 rewards 222.8\n",
            "Episode 3650/5000, average last 10 rewards 528.0\n",
            "Episode 3660/5000, average last 10 rewards -540.0\n",
            "Episode 3670/5000, average last 10 rewards 469.0\n",
            "Episode 3680/5000, average last 10 rewards 18.0\n",
            "Episode 3690/5000, average last 10 rewards 199.0\n",
            "Episode 3700/5000, average last 10 rewards -342.0\n",
            "Episode 3710/5000, average last 10 rewards 210.0\n",
            "Episode 3720/5000, average last 10 rewards 60.0\n",
            "Episode 3730/5000, average last 10 rewards 660.8\n",
            "Episode 3740/5000, average last 10 rewards 238.6\n",
            "Episode 3750/5000, average last 10 rewards -192.0\n",
            "Episode 3760/5000, average last 10 rewards -107.4\n",
            "Episode 3770/5000, average last 10 rewards 240.0\n",
            "Episode 3780/5000, average last 10 rewards 252.0\n",
            "Episode 3790/5000, average last 10 rewards -509.4\n",
            "Episode 3800/5000, average last 10 rewards 270.8\n",
            "Episode 3810/5000, average last 10 rewards -288.0\n",
            "Episode 3820/5000, average last 10 rewards 1843.0\n",
            "Episode 3830/5000, average last 10 rewards 427.0\n",
            "Episode 3840/5000, average last 10 rewards 516.6\n",
            "Episode 3850/5000, average last 10 rewards 267.0\n",
            "Episode 3860/5000, average last 10 rewards 103.0\n",
            "Episode 3870/5000, average last 10 rewards -376.6\n",
            "Episode 3880/5000, average last 10 rewards 169.0\n",
            "Episode 3890/5000, average last 10 rewards -184.8\n",
            "Episode 3900/5000, average last 10 rewards 356.0\n",
            "Episode 3910/5000, average last 10 rewards 546.0\n",
            "Episode 3920/5000, average last 10 rewards 660.0\n",
            "Episode 3930/5000, average last 10 rewards -6.0\n",
            "Episode 3940/5000, average last 10 rewards -268.4\n",
            "Episode 3950/5000, average last 10 rewards 60.8\n",
            "Episode 3960/5000, average last 10 rewards 660.0\n",
            "Episode 3970/5000, average last 10 rewards 0.0\n",
            "Episode 3980/5000, average last 10 rewards -51.2\n",
            "Episode 3990/5000, average last 10 rewards -461.2\n",
            "Episode 4000/5000, average last 10 rewards 181.0\n",
            "Episode 4010/5000, average last 10 rewards 414.8\n",
            "Episode 4020/5000, average last 10 rewards 42.8\n",
            "Episode 4030/5000, average last 10 rewards 330.0\n",
            "Episode 4040/5000, average last 10 rewards 708.0\n",
            "Episode 4050/5000, average last 10 rewards 378.0\n",
            "Episode 4060/5000, average last 10 rewards 583.2\n",
            "Episode 4070/5000, average last 10 rewards 162.0\n",
            "Episode 4080/5000, average last 10 rewards 342.8\n",
            "Episode 4090/5000, average last 10 rewards -103.0\n",
            "Episode 4100/5000, average last 10 rewards -618.0\n",
            "Episode 4110/5000, average last 10 rewards 486.0\n",
            "Episode 4120/5000, average last 10 rewards 426.6\n",
            "Episode 4130/5000, average last 10 rewards 12.0\n",
            "Episode 4140/5000, average last 10 rewards -419.2\n",
            "Episode 4150/5000, average last 10 rewards 132.6\n",
            "Episode 4160/5000, average last 10 rewards 300.0\n",
            "Episode 4170/5000, average last 10 rewards 379.2\n",
            "Episode 4180/5000, average last 10 rewards -188.2\n",
            "Episode 4190/5000, average last 10 rewards 444.0\n",
            "Episode 4200/5000, average last 10 rewards 660.0\n",
            "Episode 4210/5000, average last 10 rewards 511.0\n",
            "Episode 4220/5000, average last 10 rewards 192.0\n",
            "Episode 4230/5000, average last 10 rewards 793.2\n",
            "Episode 4240/5000, average last 10 rewards 271.2\n",
            "Episode 4250/5000, average last 10 rewards 420.0\n",
            "Episode 4260/5000, average last 10 rewards 710.0\n",
            "Episode 4270/5000, average last 10 rewards 72.6\n",
            "Episode 4280/5000, average last 10 rewards 446.4\n",
            "Episode 4290/5000, average last 10 rewards -54.0\n",
            "Episode 4300/5000, average last 10 rewards -120.0\n",
            "Episode 4310/5000, average last 10 rewards 235.6\n",
            "Episode 4320/5000, average last 10 rewards 90.4\n",
            "Episode 4330/5000, average last 10 rewards -210.0\n",
            "Episode 4340/5000, average last 10 rewards 191.0\n",
            "Episode 4350/5000, average last 10 rewards -744.0\n",
            "Episode 4360/5000, average last 10 rewards 179.2\n",
            "Episode 4370/5000, average last 10 rewards 276.0\n",
            "Episode 4380/5000, average last 10 rewards -352.2\n",
            "Episode 4390/5000, average last 10 rewards -270.0\n",
            "Episode 4400/5000, average last 10 rewards -42.0\n",
            "Episode 4410/5000, average last 10 rewards 242.4\n",
            "Episode 4420/5000, average last 10 rewards 192.0\n",
            "Episode 4430/5000, average last 10 rewards 505.0\n",
            "Episode 4440/5000, average last 10 rewards -253.0\n",
            "Episode 4450/5000, average last 10 rewards -215.0\n",
            "Episode 4460/5000, average last 10 rewards 834.4\n",
            "Episode 4470/5000, average last 10 rewards 888.0\n",
            "Episode 4480/5000, average last 10 rewards 145.0\n",
            "Episode 4490/5000, average last 10 rewards 18.8\n",
            "Episode 4500/5000, average last 10 rewards 504.2\n",
            "Episode 4510/5000, average last 10 rewards 222.0\n",
            "Episode 4520/5000, average last 10 rewards 662.6\n",
            "Episode 4530/5000, average last 10 rewards -36.0\n",
            "Episode 4540/5000, average last 10 rewards 283.2\n",
            "Episode 4550/5000, average last 10 rewards 121.2\n",
            "Episode 4560/5000, average last 10 rewards 733.2\n",
            "Episode 4570/5000, average last 10 rewards 91.0\n",
            "Episode 4580/5000, average last 10 rewards 337.2\n",
            "Episode 4590/5000, average last 10 rewards 391.0\n",
            "Episode 4600/5000, average last 10 rewards 49.2\n",
            "Episode 4610/5000, average last 10 rewards -72.0\n",
            "Episode 4620/5000, average last 10 rewards 839.6\n",
            "Episode 4630/5000, average last 10 rewards -142.8\n",
            "Episode 4640/5000, average last 10 rewards 630.0\n",
            "Episode 4650/5000, average last 10 rewards 388.8\n",
            "Episode 4660/5000, average last 10 rewards 510.0\n",
            "Episode 4670/5000, average last 10 rewards 378.0\n",
            "Episode 4680/5000, average last 10 rewards 132.0\n",
            "Episode 4690/5000, average last 10 rewards -545.4\n",
            "Episode 4700/5000, average last 10 rewards 464.2\n",
            "Episode 4710/5000, average last 10 rewards 619.0\n",
            "Episode 4720/5000, average last 10 rewards 171.4\n",
            "Episode 4730/5000, average last 10 rewards 318.0\n",
            "Episode 4740/5000, average last 10 rewards -281.4\n",
            "Episode 4750/5000, average last 10 rewards -58.8\n",
            "Episode 4760/5000, average last 10 rewards -77.2\n",
            "Episode 4770/5000, average last 10 rewards 372.0\n",
            "Episode 4780/5000, average last 10 rewards -419.0\n",
            "Episode 4790/5000, average last 10 rewards 54.0\n",
            "Episode 4800/5000, average last 10 rewards -240.0\n",
            "Episode 4810/5000, average last 10 rewards 97.0\n",
            "Episode 4820/5000, average last 10 rewards 180.0\n",
            "Episode 4830/5000, average last 10 rewards 523.0\n",
            "Episode 4840/5000, average last 10 rewards -269.2\n",
            "Episode 4850/5000, average last 10 rewards -930.0\n",
            "Episode 4860/5000, average last 10 rewards -400.8\n",
            "Episode 4870/5000, average last 10 rewards -40.4\n",
            "Episode 4880/5000, average last 10 rewards 0.6\n",
            "Episode 4890/5000, average last 10 rewards -398.6\n",
            "Episode 4900/5000, average last 10 rewards -449.0\n",
            "Episode 4910/5000, average last 10 rewards 420.0\n",
            "Episode 4920/5000, average last 10 rewards -90.0\n",
            "Episode 4940/5000, average last 10 rewards -318.0\n",
            "Episode 4950/5000, average last 10 rewards 78.8\n",
            "Episode 4960/5000, average last 10 rewards -295.0\n",
            "Episode 4970/5000, average last 10 rewards -107.2\n",
            "Episode 4980/5000, average last 10 rewards -533.4\n",
            "Episode 4990/5000, average last 10 rewards 434.0\n",
            "Episode 5000/5000, average last 10 rewards -83.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def interact_env(state, model, epsilon, n_actions, env):\n",
        "  # Preprocess state\n",
        "  state_array = np.array(state) \n",
        "  state = tf.convert_to_tensor([state_array])\n",
        "\n",
        "  # Sample action\n",
        "  coin = np.random.random()\n",
        "  if coin > epsilon:\n",
        "      q_vals = model([state])\n",
        "      action = int(tf.argmax(q_vals[0]).numpy())\n",
        "  else:\n",
        "      action = np.random.choice(n_actions)\n",
        "\n",
        "\n",
        "  if env.state.Ra == 0 and env.state.U == 0:\n",
        "      action = 0\n",
        "  if env.state.U > 0:\n",
        "      action = 0\n",
        "  if env.sendbackaction == True:\n",
        "      action = 1\n",
        "  # Apply sampled action in the environment, receive reward and next state\n",
        "  next_state, reward, done = env.step(action)\n",
        "  if next_state[1] == \"Ch1\":\n",
        "      next_state[1] = 1\n",
        "  else:\n",
        "      next_state[1] = 0\n",
        "  next_state = next_state.astype(np.float32)\n",
        "  interaction = {'state': state_array, 'action': action, 'next_state': next_state.copy(),\n",
        "                'reward': reward, 'done':np.float32(done)}\n",
        "\n",
        "  return interaction"
      ],
      "metadata": {
        "id": "I4pM1SnPwvr0"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "# from Environment import Actor\n",
        "from Rudder import LessonBuffer\n",
        "from Environment import Environment\n",
        "from Rudder import RRLSTM as LSTM\n",
        "import torch\n",
        "import time as Time\n",
        "import random\n",
        "from PolicyUpdater import PolicyUpdater\n",
        "from tqdm import tqdm\n",
        "from collections import deque, defaultdict\n",
        "\n",
        "\n",
        "\n",
        "environment = Environment(1000,100)\n",
        "environment.CreateStates()\n",
        "episode = 0\n",
        "for i in (range(100)):\n",
        "    episode += 1\n",
        "    environment.reset_paramter()\n",
        "    state, _ = environment.reset_state()\n",
        "    if state[1] == \"Ch1\":\n",
        "        state[1] = 1\n",
        "    else:\n",
        "        state[1] = 0\n",
        "\n",
        "    state = state.astype(np.float32)\n",
        "    state_array = np.array(state) \n",
        "    state = tf.convert_to_tensor([state_array])\n",
        "    print(state)\n",
        "    q_vals = model([state])\n",
        "    action = int(tf.argmax(q_vals[0]).numpy())\n",
        "    print(q_vals)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qwn7nfZAd4n",
        "outputId": "8f464a95-274e-4260-a1ac-28f25c67c612"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[186.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[210.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[254.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 66.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[194.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[142.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[202.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[314.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 22.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[250.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 74.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 70.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[258.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[186.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[190.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[258.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[194.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[146.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[150.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 66.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[202.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 70.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[210.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[150.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[202.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[326.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[306.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[262.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 74.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 30.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[258.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 22.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[210.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[250.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 22.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[202.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[150.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[146.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[198.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[310.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[190.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[246.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[266.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[126.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[198.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[138.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[330.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[258.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 74.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 74.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[306.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[310.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 78.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[326.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[326.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[130.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 30.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[270.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[262.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[306.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[190.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 86.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[190.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[134.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[142.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[126.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[326.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[146.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[270.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[322.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[330.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[270.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[202.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[318.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[250.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[186.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 30.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[314.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 82.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[310.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[258.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 78.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[314.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[150.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[310.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[210.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[258.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 74.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[310.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 70.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[130.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[206.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[246.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[270.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[202.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 22.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[134.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[258.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[198.   0. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.54816 158.02353]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[262.   1. 500.   1.   0.]], shape=(1, 5), dtype=float32)\n",
            "tf.Tensor([[175.85338 158.68065]], shape=(1, 2), dtype=float32)\n"
          ]
        }
      ]
    }
  ]
}